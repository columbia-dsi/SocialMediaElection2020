---
title: "EDAV Fall 2019 Final Project: Understanding 2020 Presidential Polls from Social Media Perspective"
author: "Kevin Gao (wg2311), Haibo Yu (hy2628)"
output: html_document
---
```{r, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = TRUE)
```

This assignment is designed to help you get started on the final project. Be sure to review the final project instructions (https://edav.info/project.html), in particular the new section on reproducible workflow (https://edav.info/project.html#reproducible-workflow), which summarizes principles that we've discussed in class.
    
### 1. The Team

[2 points]

a) Who's on the team? (Include names and UNIs)

Haibo Yu - hy2628

Kevin Gao - wg2311

b) How do you plan to divide up the work? (Grading is on a group basis. The point of asking is to encourage you to think about this.)

Haibo: 
1. Data collecting: Wrote a python script to do the web scraping work for youtube, facebook, twitter. 
2. Data preprocessing: Clean up missing data, unified data and merge into csv.
3. Data analysis: Data exploration and visualization

Kevin: 
1. Data pipeline: Design, and architect the data flow and project details.
2. Data analysis: Data exploration and visualization
3. Data prediction: Apply machine learning, NLP, GLM to make predictions of our data.


### 2. The Questions

[6 points]

List three questions that you hope you will be able to answer from your research.


###### Core questions:

a) In each state, which candidate has more popularity online, in terms of views, likes, retweets...etc ?

b) For each candidate, in which state he/she has more popularity online, in terms of views, likes, retweets...etc ?

c) In general, which candidate has more popularity online ?


###### Potentially, we would like to answer these questions:

d) Who is more concentrating on what topics ?

e) Who is more popular in near future ?

f) Is our result correlating to the final election result in December ?





### 3. Which output format do you plan to use to submit the project? 



[2 points]

(You don't have to use the same format for this assignment -- PSet 5, part A -- and the final project itself.)

Choices are: ( Y - Yes, N - No )


[Y] html_document  

[Y] pdf_document  

[Y] bookdown book: https://bookdown.org/yihui/bookdown/

[N] shiny app: https://shiny.rstudio.com/  


(Remember that it's ok to have pieces of the project that don't fit into the chosen output format; in those cases you can provide links to the relevant material.)

### 4. The Data

What is your data source?  What is your method for importing data? Please be specific. Provide relevant information such as any obstacles you're encountering and what you plan to do to overcome them.

[5 points]
Answer:

Our data source is original, collected from social meida networks, including youtube, facebook, twitter.

Methodology: We wrote a python script (web crawler) to do the web scraping work to collect data.

Obstacle: There certainly many obstacles in a project, however, here is an interesting one, when we were scraping search result data from youtube for each candidate, we faced with obstacles when grabing the full data from the lazy loading result page, which would hide large content and load those later by user scrolling down to the bottom of the page, this actually prevented us to rely on the initial html dom to scrap the full data. And it's hard to use web scrawler in python to control the lazy loading behavior.

Solution: After serveral attempts, we have resolved this by using a walk-around solution provided in youtube search feature, by adding a paramter '&page=N' (N is the number you would like to paginate) to laod all search result at once, then we can scrap the data page by page.



### 5. Provide a short summary, including 2-3 graphs, of your initial investigations. 

[10 points]

```{r, warning=FALSE}
library(tidyverse)
library(dplyr)
library(choroplethr)
library(ggplot2)


# load social meida data - Youtube, from 11/15/2019:
df <- read.csv("2019-11-15.csv")

# load social meida data - Youtube, from 12/12/2019:
df2 <- read.csv("2019-12-12.csv")

# toString(df2$time)
# typeof(df2$time)
# 
# as.Date(paste("20",c(unlist(strsplit(toString(df2$time),"/")))[3],"-",c(unlist(strsplit(toString(df2$time),"/")))[1],"-",c(unlist(strsplit(toString(df2$time),"/")))[2],sep=""), format = "%Y-%m-%d")
# 
# tt <- strsplit(toString(df2$time[0]),"/")
# xx <- df2$time[1]
# 
# df2 <- filter(df2, as.Date(paste("20",c(unlist(strsplit(toString(time),"/")))[3],"-",c(unlist(strsplit(toString(time),"/")))[1],"-",c(unlist(strsplit(toString(time),"/")))[2],sep=""), format = "%Y-%m-%d") > as.Date("2019-1-1", format = "%Y-%m-%d")) 


df <- filter(df, candidate != "Donald Trump") 


df2 <- filter(df2, candidate != "Donald Trump")

df2 <- filter(df2, title!="2020 Democratic Debate - SNL")  # remove unrelated videos

df2 <- filter(df2, title!="DNC Town Hall - SNL")  # remove unrelated videos

df2 <- filter(df2, title!="2020 November Democratic Debate in Atlanta | The Daily Show")  # remove unrelated videos

df2 <- filter(df2, title!="2020 October Democratic Debate in Ohio | The Daily Show")  # remove unrelated videos

df2 <- filter(df2, title!="Stephen Colbert Breaks Down The 5th Democratic Presidential Debate")  # remove unrelated videos

df2 <- filter(df2, title!="Democratic candidates debate: Opening statements l ABC News")  # remove unrelated videos


# df <- filter(df, grepl(candidate, title, fixed=FALSE)) 

# df2 <- filter(df2, grepl(toString(candidate), toString(title), fixed=TRUE)) #remove due to huge lost of data


# Intially, use one date point (on 11/15/2019), we would like to visualize and rank these total popularity of each candiates on youtube:


head(df)

# Ploting:
summary(df$candidate)
summary(df2$candidate)
# plotting

temp <- 
  df %>%
  group_by(candidate) %>%
  summarise(view=sum(as.numeric(view)))


ggplot(temp, aes(x = reorder(candidate, view), y=view)) +
  geom_bar(stat="identity") +
  xlab("Candidate Name") +
  ylab("Views") +
  ggtitle("Total Youtube Views") +
  coord_flip()



# We also would like to visualize these 2 questions in one series of plots using choropleth maps:

## a) In each state, which candidate has more popularity online, in terms of views, likes, retweets...etc ?
## b) For each candidate, in which state he/she has more popularity online, in terms of views, likes, retweets...etc ?


candidates <- c('Amy Klobuchar','Andrew Yang','Bernie Sanders','Cory Booker','Elizabeth Warren','Joe Biden','Kamala Harris','Pete Buttigieg','Tom Steyer','Tulsi Gabbard')

for(i in candidates) {
  # i-th name of candiates
  # ------------------------------------------------------
  temp <- filter(df2, candidate == i) 
  
  temp <- temp %>% 
    as.data.frame() %>% 
    transmute(region = tolower(`state`), value = as.numeric(temp$view))
  
  # unique(df$region)  45 states
  
  temp <- temp %>%
    group_by(region) %>%
    summarise(value = sum(value))
  
  temp <- na.omit(temp)
  
  print(state_choropleth(temp,
                   title = paste(i," - State Youtube Views"),
                   legend = "View Count"))
  
  # ------------------------------------------------------
    
}

```


## Initial investigations:
#### 1. In general, we can see the current candidates' popularity ranking on youtube is:
   Bernie Sanders > Joe Biden > Elizabeth Warren > Andrew Yang > Kamala Harris > Pete Buttigieg > Tulsi Gabbard > Cory Booker > Amy Kilobuchar > Tom Steyer
   
#### 2. The state series choropleth maps, we can see the top candidate like Bernie Sanders, Joe Biden and Elizabeth Warren has less missing data across states, on the other hand, lower ranking candidates have more missing data, which indicates fewer views and less popular in youtube.

#### 3, In general, most views across all candiate appear both east and west coast, major urban cities, which corrlates that peopel from the 11 swing states watch less youtube videos of these democrat candidates.


## Topic Analysis :


Note: Here are a couple of important definitions.

scale: Indicates the range of sizes of the words.
max.words: Plots the specified number of words and discard least frequent terms.
min.freq: Discards all terms whose frequency is below the specified value.
random.order: By setting this to false, we make it so that the words with the highest frequency are plotted first. If we donâ€™t set this, it will plot the words in a random order and the highest frequency words may not necessarily appear in the center.
rot.per: Determines the fraction of words that are plotted vertically.
colors: The default value is black. If you want to use different colors based on frequency, you can specify a vector of colors or use one of the pre-defined color palettes.

```{r, warning=FALSE}
library(tm)
library(SnowballC)
library(RColorBrewer)
library(wordcloud)

# Calculate Corpus
videoDS.Corpus<-Corpus(VectorSource(df2$description))


# Data Cleaning and Wrangling
videoDS.Clean<-tm_map(videoDS.Corpus, PlainTextDocument)
videoDS.Clean<-tm_map(videoDS.Corpus,tolower)
videoDS.Clean<-tm_map(videoDS.Clean,removeNumbers)
videoDS.Clean<-tm_map(videoDS.Clean,removeWords,stopwords("english"))
videoDS.Clean<-tm_map(videoDS.Clean,removeWords,stopwords("spanish"))
videoDS.Clean<-tm_map(videoDS.Clean,removePunctuation)
videoDS.Clean<-tm_map(videoDS.Clean,stripWhitespace)
videoDS.Clean<-tm_map(videoDS.Clean,stemDocument)


# wordcloud(videoDS.Clean,max.words = 1000,random.color = TRUE,random.order=FALSE)
layout(matrix(c(1, 2), nrow=2), heights=c(1, 4))
par(mar=rep(0, 4))
plot.new()
text(x=0.5, y=0.5, cex= 1.5, "Topics from Candidate - All", font = 2, col="darkgray")
wordcloud(words = videoDS.Clean, min.freq = 10,
          max.words=1000, random.order=FALSE, rot.per=0.25, 
          colors=brewer.pal(10, "Dark2"))


candidates <- c('Amy Klobuchar','Andrew Yang','Bernie Sanders','Cory Booker','Elizabeth Warren','Joe Biden','Kamala Harris','Pete Buttigieg','Tom Steyer','Tulsi Gabbard')

for(i in candidates) {
  # i-th name of candiates
  # ------------------------------------------------------
  temp <- filter(df2, candidate == i) 
  
  # Calculate Corpus
  videoDS.Corpus<-Corpus(VectorSource(temp$description))
  
  
  # Data Cleaning and Wrangling
  videoDS.Clean<-tm_map(videoDS.Corpus, PlainTextDocument)
  videoDS.Clean<-tm_map(videoDS.Corpus,tolower)
  videoDS.Clean<-tm_map(videoDS.Clean,removeNumbers)
  videoDS.Clean<-tm_map(videoDS.Clean,removeWords,stopwords("english"))
  videoDS.Clean<-tm_map(videoDS.Clean,removeWords,stopwords("spanish"))
  videoDS.Clean<-tm_map(videoDS.Clean,removePunctuation)
  videoDS.Clean<-tm_map(videoDS.Clean,stripWhitespace)
  videoDS.Clean<-tm_map(videoDS.Clean,stemDocument)
  
  
  # wordcloud(videoDS.Clean,max.words = 1000,random.color = TRUE,random.order=FALSE)
  layout(matrix(c(1, 2), nrow=2), heights=c(1, 4))
  par(mar=rep(0, 4))
  plot.new()
  text(x=0.5, y=0.5, cex= 1.5, paste("Topics from Candidate - ", i, sep=""), font = 2, col="darkgray")
  wordcloud(words = videoDS.Clean, min.freq = 100,
            max.words=1000, random.order=FALSE, rot.per=0.35, 
            colors=brewer.pal(8, "Dark2"))
  # ------------------------------------------------------
}
```


## Diverage Candidates popularity:
```{r, warning=FALSE}
library(HH)


temp <- 
  df2 %>%
  group_by(candidate) %>%
  summarise(dilike=sum(as.numeric(dislike)), like=sum(as.numeric(like)))


HH::likert(candidate ~ ., temp, xlab = 'Percent',
           main = list('Diverage Candidates Popularity - Absolute Value', cex = 0.7), positive.order = T, col=c('#6f8df7','#f582ae'))



temp2 <- 
  df2 %>%
  group_by(candidate) %>%
  summarise(dilike=sum(as.numeric(dislike))/(sum(as.numeric(dislike))+sum(as.numeric(like))), like=sum(as.numeric(like))/(sum(as.numeric(dislike))+sum(as.numeric(like))))


HH::likert(candidate ~ ., temp2, xlab = 'Percent',
           main = list('Diverage Candidates Popularity - Percentage', cex = 0.7), positive.order = T, col=c('#6f8df7','#f582ae'))

```


## Comments Analysis :
```{r, warning=FALSE}

library(tm)
library(SnowballC)
library(RColorBrewer)
library(wordcloud)

library(HH)

# load social meida data - Youtube, from 12/12/2019:
df3 <- read.csv("2019-12-12_comment.csv")
summary(df3$candidate)

candidates <- c('Amy Klobuchar','Andrew Yang','Bernie Sanders','Cory Booker','Elizabeth Warren','Joe Biden','Kamala Harris','Pete Buttigieg','Tom Steyer','Tulsi Gabbard')

for(i in candidates) {
  # i-th name of candiates
  # ------------------------------------------------------
  temp <- filter(df3, candidate == i) 
  
  # Calculate Corpus
  videoDS.Corpus<-Corpus(VectorSource(temp$comment))
  
  
  # Data Cleaning and Wrangling
  videoDS.Clean<-tm_map(videoDS.Corpus, PlainTextDocument)
  videoDS.Clean<-tm_map(videoDS.Corpus,tolower)
  videoDS.Clean<-tm_map(videoDS.Clean,removeNumbers)
  videoDS.Clean<-tm_map(videoDS.Clean,removeWords,stopwords("english"))
  videoDS.Clean<-tm_map(videoDS.Clean,removeWords,stopwords("spanish"))
  videoDS.Clean<-tm_map(videoDS.Clean,removePunctuation)
  videoDS.Clean<-tm_map(videoDS.Clean,stripWhitespace)
  videoDS.Clean<-tm_map(videoDS.Clean,stemDocument)
  
  
  # wordcloud(videoDS.Clean,max.words = 1000,random.color = TRUE,random.order=FALSE)
  layout(matrix(c(1, 2), nrow=2), heights=c(1, 4))
  par(mar=rep(0, 4))
  plot.new()
  text(x=0.5, y=0.5, cex= 1.5, paste("Comments for Candidate - ", i, sep=""), font = 2, col="darkgray")
  wordcloud(words = videoDS.Clean, min.freq = 200,
            max.words=500, random.order=FALSE, rot.per=0.35, 
            colors=brewer.pal(8, "Dark2"))
  
  # ------------------------------------------------------
}


## Sentiment Score

hu.liu.pos = readLines('https://www.dropbox.com/sh/3xctszdxx4n00xq/AAA_Go_Y3kJxQACFaVBem__ea/positive-words.txt?dl=1');
hu.liu.neg = readLines('https://www.dropbox.com/sh/3xctszdxx4n00xq/AABTGWHitlRZcddq1pPXOSqca/negative-words.txt?dl=1');

score.sentiment = function(sentences, candidate, pos.words, neg.words, .progress='none')
  {
    require(plyr);
    require(stringr);
    scores = laply(sentences, function(sentence, pos.words, neg.words) {
      sentence = gsub('[^A-z ]','', sentence)
      sentence = tolower(sentence);
      word.list = str_split(sentence, '\\s+');
      words = unlist(word.list);
      pos.matches = match(words, pos.words);
      neg.matches = match(words, neg.words);
      pos.matches = !is.na(pos.matches);
      neg.matches = !is.na(neg.matches);
      score = sum(pos.matches) - sum(neg.matches);
      return(score);
    }, pos.words, neg.words, .progress=.progress );
    scores.df = data.frame(score=scores, text=sentences, candidate=candidate);
    return(scores.df);
}

#Test
# sample=c("You're awesome and I love you","I hate and hate and hate. So angry. Die!","Impressed and amazed: you are peerless in your achievement of unparalleled mediocrity.")
# result=score.sentiment(sample, 'TEST', hu.liu.pos,hu.liu.neg)
# 

candidates <- c('Amy Klobuchar','Andrew Yang','Bernie Sanders','Cory Booker','Elizabeth Warren','Joe Biden','Kamala Harris','Pete Buttigieg','Tom Steyer','Tulsi Gabbard')

df_merge <- data.frame()


for(i in candidates) {
  # i-th name of candiates
  # ------------------------------------------------------
  temp <- filter(df3, candidate == i) 
  
  # Calculate Corpus
  result=score.sentiment(temp$comment, i, hu.liu.pos,hu.liu.neg)
  
  result <- result %>%
    group_by(candidate) %>%
    summarise(sentiment=sum(as.numeric(score)))
  
  df_merge <- rbind(df_merge, result)
  # ------------------------------------------------------
}

# ?
# temp <- 
#   df_merge %>%
#   group_by(candidate) %>%
#   summarise(sentiment=s(as.numeric(score)))

ggplot(df_merge, aes(x = reorder(candidates, sentiment), y=sentiment)) +
  geom_bar(stat="identity", fill=c('#f582ae','#f582ae','#f582ae','#f582ae','#f582ae','#f582ae','#6f8df7','#f582ae','#f582ae','#f582ae')) +
  xlab("Candidate Name") +
  ylab("Sentiment Score") +
  ggtitle("Total Sentiment Score (Positive: Red, Negative: Blue)") +
  coord_flip()

```
 
# Demographics Analysis
```{r, warning=FALSE}
## HTML embeded
```